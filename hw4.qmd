---
title: "Homework Assignment 4"
author: "Nicholas Pfeifer"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
---

# Exercise 8

NYC Crash Data Cleaning The NYC motor vehicle collisions data with
documentation is available from NYC Open Data. The raw data needs some
cleaning.

## Part a.

Use the filter from the website to download the crash data of the
week of June 30, 2024 in CSV format; save it under a directory data with an
informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data
into a Panda data frame with careful handling of the date time variables.

```{python}
import warnings
warnings.filterwarnings("ignore")
```

```{python}
import numpy as np
import pandas as pd

crash_df = pd.read_csv('data/nyccrashes_2024w0630_by20250212.csv',
                       dtype = {'ZIP CODE': str,
                                'LATITUDE': np.float32,
                                'LONGITUDE': np.float32})

crash_df
```

## Part b.

Clean up the variable names. Use lower cases and replace spaces with
underscores.

```{python}
crash_df.columns = crash_df.columns.str.lower().str.replace(' ', '_')

crash_df.head()
```

## Part c.

Check the crash date and time to see if they really match the
filter we intented. Remove the extra rows if needed.

```{python}
crash_df[crash_df['crash_date'] == '07/07/2024']
```

There are rows from July 7 that were not intended to be included.

```{python}
crash_df = crash_df[crash_df['crash_date'] != '07/07/2024']
crash_df
```

Now these rows have been removed from the dataframe.

## Part d.

Get the basic summaries of each variables: missing percentage;
descriptive statistics for continuous variables; frequency tables for
discrete variables.

**Missing Percentage**

```{python}
crash_df.isnull().mean()
```

**Descriptive Statistics for Continuous Variables**

```{python}
continuous_variables = ['latitude', 'longitude', 'number_of_persons_injured',
'number_of_persons_killed', 'number_of_pedestrians_injured',
'number_of_pedestrians_killed', 'number_of_cyclist_injured',
'number_of_cyclist_killed', 'number_of_motorist_injured',
'number_of_motorist_killed']
```

```{python}
crash_df[continuous_variables[:3]].describe()
```

```{python}
crash_df[continuous_variables[3:6]].describe()
```

```{python}
crash_df[continuous_variables[6:9]].describe()
```
```{python}
crash_df[continuous_variables[9:]].describe()
```

**Frequency Tables for Discrete Variables**

```{python}
discrete_variables = crash_df.drop(columns = continuous_variables).columns

for var in discrete_variables:
  var_freq = crash_df[var].value_counts(dropna=False).reset_index()
  var_freq.columns = [var, 'count']
  print(var_freq)
```

## Part e.

Are their invalid longitude and latitude in the data? If so,
replace them with NA.

```{python}
crash_df[(crash_df['latitude'] == 0) | (crash_df['longitude'] == 0)]
```

There are 3 rows that have longitude and latitude values of 0.
Those coordinates not in New York City, so they are invalid.

```{python}
crash_df['latitude'] = crash_df['latitude'].replace(0, np.nan)
crash_df['longitude'] = crash_df['longitude'].replace(0, np.nan)
```

```{python}
crash_df.loc[[18, 378, 1560]]
```

## Part f.

Are there zip_code values that are not legit NYC zip codes? If so,
replace them with NA.

```{python}
valid_zips_codes = {
    10463, 10471, 10466, 10469, 10470, 10475, 10458, 10467, 10468,
    10461, 10462, 10464, 10465, 10472, 10473, 10453, 10457, 10460,
    10451, 10452, 10456, 10454, 10455, 10459, 10474, 11211, 11222,
    11201, 11205, 11215, 11217, 11231, 11213, 11212, 11216, 11233,
    11238, 11207, 11208, 11220, 11232, 11204, 11218, 11219, 11230,
    11203, 11210, 11225, 11226, 11234, 11236, 11239, 11209, 11214,
    11228, 11223, 11224, 11229, 11235, 11206, 11221, 11237, 10031,
    10032, 10033, 10034, 10040, 10026, 10027, 10030, 10037, 10039,
    10029, 10035, 10023, 10024, 10025, 10021, 10028, 10044, 10128,
    10001, 10011, 10018, 10019, 10020, 10036, 10010, 10016, 10017,
    10022, 10012, 10013, 10014, 10002, 10003, 10009, 10004, 10005,
    10006, 10007, 10038, 10280, 11101, 11102, 11103, 11104, 11105,
    11106, 11368, 11369, 11370, 11372, 11373, 11377, 11378, 11354,
    11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363,
    11364, 11374, 11375, 11379, 11385, 11365, 11366, 11367, 11414,
    11415, 11416, 11417, 11418, 11419, 11420, 11421, 11412, 11423,
    11432, 11433, 11434, 11435, 11436, 11004, 11005, 11411, 11413,
    11422, 11426, 11427, 11428, 11429, 11691, 11692, 11693, 11694,
    11695, 11697, 10302, 10303, 10310, 10301, 10304, 10305, 10314,
    10306, 10307, 10308, 10309, 10312
}

crash_df['zip_code_numeric'] = pd.to_numeric(crash_df['zip_code'],
                                             errors='coerce')

invalid_zip_codes = crash_df[crash_df['zip_code_numeric'].isna() |
  ~crash_df['zip_code_numeric'].isin(valid_zips_codes)]['zip_code']

invalid_zip_freq = invalid_zip_codes.value_counts(dropna=False)
invalid_zip_freq.columns = ['zip_code', 'frequency']

invalid_zip_freq
```

```{python}
crash_df[~crash_df['zip_code_numeric'].isin(valid_zips_codes)
         & ~crash_df['zip_code_numeric'].isna()]
```

The rows with invalid zip codes are shown above.

The code below replaces invalid zip codes with NAs. I decided it
was more practical to run this step after part g.

```{python}
'''crash_df['zip_code'] = crash_df['zip_code'].replace(['10065', '11249',
                                                     '10112', '11040'],
                                                     np.nan)

invalid_zip_codes = crash_df[crash_df['zip_code_numeric'].isna() |
  ~crash_df['zip_code_numeric'].isin(valid_zips_codes)]['zip_code']

invalid_zip_freq = invalid_zip_codes.value_counts(dropna=False)
invalid_zip_freq.columns = ['zip_code', 'frequency']

invalid_zip_freq'''
```

## Part g.

Are there missing in zip_code and borough? Do they always co-occur?

```{python}
missing_zip = crash_df['zip_code'].isnull().sum()
missing_borough = crash_df['borough'].isnull().sum()

missing_both = crash_df[['zip_code', 'borough']].isnull().all(axis=1).sum()

(missing_zip, missing_borough, missing_both)
```

The missing values in zip_code and borough do always co-occur.

### Part f. continued

```{python}
crash_df['zip_code'] = crash_df['zip_code'].replace(['10065', '11249',
                                                     '10112', '11040'],
                                                     np.nan)

invalid_zip_codes = crash_df[crash_df['zip_code_numeric'].isna() |
  ~crash_df['zip_code_numeric'].isin(valid_zips_codes)]['zip_code']

invalid_zip_freq = invalid_zip_codes.value_counts(dropna=False)
invalid_zip_freq.columns = ['zip_code', 'frequency']

invalid_zip_freq
```

The invalid zip codes have now been converted to NAs.

## Part h.

Are there cases where zip_code and borough are missing but the geo codes are
not missing? If so, fill in zip_code and borough using the geo codes.

```{python}
nozip_locs = crash_df[crash_df['zip_code'].isna() & \
                      crash_df['borough'].isna() & \
                      ~crash_df['latitude'].isna() & \
                      ~crash_df['longitude'].isna()]
nozip_locs
```

The rows above are missing zip_code and borough, but have valid
latitude and longitude.

```{python}
from geopy.geocoders import Nominatim
import time

geolocator = Nominatim(user_agent="jyGeopyTry")
```

```{python}
def get_zip_code(latitude, longitude):
    try:
        location = geolocator.reverse((latitude, longitude), timeout=10)
        if location:
            address = location.raw['address']
            zip_code = address.get('postcode', None)
            return zip_code
        else:
            return None
    except Exception as e:
        print(f"Error: {e} for coordinates {latitude}, {longitude}")
        return None
    finally:
        time.sleep(0.15)
```

The following code block takes approximately 6 minutes and 17 seconds to run,
so I saved the data as a csv instead of running it over and over again.

```{python}
'''from tqdm import tqdm

for idx, row in tqdm(nozip_locs.iterrows(), total=nozip_locs.shape[0],
                     dynamic_ncols=True, ncols=10):
  zip_code = get_zip_code(row['latitude'], row['longitude'])
  crash_df.loc[idx, 'zip_code'] = zip_code'''
```

```{python}
'''crash_df.to_csv('crash_with_new_zips.csv', index = False)'''
```

```{python}
crash_df = pd.read_csv('data/crash_with_new_zips.csv')
```

```{python}
crash_df[(crash_df['borough'].isna()) & (~crash_df['latitude'].isna())]
```

Now the zip codes have been filled in, and the respective boroughs can be
derived.

Here is the borough data before derivation.

```{python}
crash_df['borough'].value_counts(dropna=False)
```

```{python}
crash_df['zip_code_numeric'] = pd.to_numeric(crash_df['zip_code'])
```

```{python}
crash_df.loc[(crash_df['zip_code_numeric'] >= 10001) & \
  (crash_df['zip_code_numeric'] <= 10282), 'borough'] = 'MANHATTAN'
crash_df.loc[(crash_df['zip_code_numeric'] >= 10301) & \
  (crash_df['zip_code_numeric'] <= 10314), 'borough'] = 'STATEN ISLAND'
crash_df.loc[(crash_df['zip_code_numeric'] >= 10451) & \
  (crash_df['zip_code_numeric'] <= 10475), 'borough'] = 'BRONX'
crash_df.loc[((crash_df['zip_code_numeric'] >= 11004) & \
  (crash_df['zip_code_numeric'] <= 11009)) | \
  ((crash_df['zip_code_numeric'] >= 11351) & \
  (crash_df['zip_code_numeric'] <= 11697)), 'borough'] = 'QUEENS'
crash_df.loc[(crash_df['zip_code_numeric'] >= 11201) & \
  (crash_df['zip_code_numeric'] <= 11256), 'borough'] = 'BROOKLYN'
```

```{python}
crash_df
```

Now the borough has been filled in.

Here is the borough data after derivation.

```{python}
crash_df['borough'].value_counts(dropna=False)
```

## Part i.

Is it redundant to keep both location and the longitude/latitude at
the NYC Open Data server?

Yes, it is redundant to keep both. We do not need two different
ways of storing the location data. Since it is easier to access the data
when logitude and latitude are in separate columns, we can get rid of the
location column which is containing both values in the same column.

```{python}
crash_df = crash_df.drop(columns = ['location'])
crash_df.head()
```

## Part j.

Check the frequency of crash_time by hour. Is there a matter of bad luck
at exactly midnight? How would you interpret this?

```{python}
crash_df['stop_index'] = crash_df['crash_time'].str.find(':')
crash_df['crash_time_hour'] = crash_df.apply(
  lambda x: x['crash_time'][:x['stop_index']], axis = 1)

crash_df['crash_time_hour'].value_counts()
```

```{python}
crash_df['crash_time'].value_counts()
```

The midnight hour is responsible for the most amount of crashes.
The second at exactly midnight also appears to be a hotspot for crashes.
I would expect crashes to be more evenly distributed across all of the
seconds in a day. An explanation for this could be that the crash time is
more likely to be reported at hourly or 5-minute intervals since some
people may find it more practical. Those involved in a 
motor vehicle collision can be distracted with their environment,
which could lead them to give a rough estimate of when the crash occured.

## Part k.

Are the number of persons killed/injured the summation of the numbers of
pedestrians, cyclist, and motorists killed/injured? If so, is it redundant
to keep these two columns at the NYC Open Data server?

```{python}
crash_df['injured_sum'] = crash_df['number_of_pedestrians_injured'] + \
                          crash_df['number_of_cyclist_injured'] + \
                          crash_df['number_of_motorist_injured']

crash_df['injured_verify'] = crash_df['number_of_persons_injured'] == \
                             crash_df['injured_sum']

crash_df['injured_verify'].value_counts()
```

```{python}
crash_df[crash_df['injured_verify'] == False][['number_of_persons_injured',
                                               'number_of_pedestrians_injured',
                                               'number_of_cyclist_injured',
                                               'number_of_motorist_injured']]
```

```{python}
crash_df['killed_sum'] = crash_df['number_of_pedestrians_killed'] + \
                          crash_df['number_of_cyclist_killed'] + \
                          crash_df['number_of_motorist_killed']

crash_df['killed_verify'] = crash_df['number_of_persons_killed'] == \
                             crash_df['killed_sum']

crash_df['killed_verify'].value_counts()
```

Interesting, the number of persons killed/injured is the summation of the
pedestrians, cyclists, and motorists columns for almost every row.
The number_of_persons_killed is the summation of the killed columns
for every row. However, for the injured columns there are 43 rows in which
the number_of_persons_injured is not the summation of the other three columns.
It would be redundant to keep
these summartion columns (if they were always equal) since they can be derived
from other columns and they take up space. But since the injuries do not add
up for every row, we would lose some information by removing the sum column.
A possible explanation is that these specific crashes did have injuries, but
it was not specified what the person's role was in the collision or their
role did not align with any of the three established columns.

## Part l.

Print the whole frequency table of contributing_factor_vehicle_1.
Convert lower cases to uppercases and check the frequencies again.

```{python}
cf1_freq = crash_df['contributing_factor_vehicle_1'].value_counts(dropna=False)
cf1_freq.columns = ['contributing_factor_vehicle_1', 'count']
cf1_freq
```

```{python}
crash_df['contributing_factor_vehicle_1'] = crash_df['contributing_factor_vehicle_1'].str.upper()

cf1_freq = crash_df['contributing_factor_vehicle_1'].value_counts(dropna=False)
cf1_freq.columns = ['contributing_factor_vehicle_1', 'count']
cf1_freq
```

These two frequency tables look the same.

## Part m.

Provided an opportunity to meet the data provider, what suggestions
would you make based on your data exploration experience?

Given the oppotunity, I would have a few suggestions. One thing that stands
out is the number of rows missing zip code and borough data relative to the
number of rows missing location data. If latitude and longitude are known,
zip code and borough should be able to be dervided. I would also suggest
verifying that all longitudes and latitudes are valid. It does not make
sense that there are rows with latitude and longitude values of 0. Assuming
that the location of the collsion is unknown, these values should be NA.
Lastly, I would suggest checking that all zip codes in the data set are valid
zip codes of New York City.