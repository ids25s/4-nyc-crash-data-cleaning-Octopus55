---
title: "Homework Assignment 4"
author: "Nicholas Pfeifer"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
---

# Exercise 8

NYC Crash Data Cleaning The NYC motor vehicle collisions data with
documentation is available from NYC Open Data. The raw data needs some
cleaning.

## Part a.

Use the filter from the website to download the crash data of the
week of June 30, 2024 in CSV format; save it under a directory data with an
informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data
into a Panda data frame with careful handling of the date time variables.

```{python}
import numpy as np
import pandas as pd

crash_df = pd.read_csv('data/nyccrashes_2024w0630_by20250212.csv',
                       dtype = {'ZIP CODE': str,
                                'LATITUDE': np.float32,
                                'LONGITUDE': np.float32})

crash_df
```

## Part b.

Clean up the variable names. Use lower cases and replace spaces with
underscores.

```{python}
crash_df.columns = crash_df.columns.str.lower().str.replace(' ', '_')

crash_df.head()
```

## Part c.

Check the crash date and time to see if they really match the
filter we intented. Remove the extra rows if needed.

## Part d.

Get the basic summaries of each variables: missing percentage;
descriptive statistics for continuous variables; frequency tables for
discrete variables.

**Missing Percentage**

```{python}
crash_df.isnull().mean()
```

**Descriptive Statistics for Continuous Variables**

```{python}
continuous_variables = ['latitude', 'longitude', 'number_of_persons_injured',
'number_of_persons_killed', 'number_of_pedestrians_injured',
'number_of_pedestrians_killed', 'number_of_cyclist_injured',
'number_of_cyclist_killed', 'number_of_motorist_injured',
'number_of_motorist_killed']

crash_df[continuous_variables].describe()
```

**Frequency Tables for Discrete Variables**

```{python}
discrete_variables = crash_df.drop(columns = continuous_variables).columns

for var in discrete_variables:
  var_freq = crash_df[var].value_counts(dropna=False)
  var_freq.columns = [var, 'count']
  print(var_freq)
```

## Part e.

Are their invalid longitude and latitude in the data? If so,
replace them with NA.

```{python}
crash_df[(crash_df['latitude'] == 0) | (crash_df['longitude'] == 0)]
```

There are 3 rows that have longitude and latitude values of 0.
Those coordinates not in New York City, so they are invalid.

```{python}
crash_df['latitude'] = crash_df['latitude'].replace(0, pd.NA)
crash_df['longitude'] = crash_df['longitude'].replace(0, pd.NA)
```

```{python}
crash_df.loc[[18, 378, 1560]]
```

## Part f.

Are there zip_code values that are not legit NYC zip codes? If so,
replace them with NA.

```{python}
valid_zips_codes = {
    10463, 10471, 10466, 10469, 10470, 10475, 10458, 10467, 10468,
    10461, 10462, 10464, 10465, 10472, 10473, 10453, 10457, 10460,
    10451, 10452, 10456, 10454, 10455, 10459, 10474, 11211, 11222,
    11201, 11205, 11215, 11217, 11231, 11213, 11212, 11216, 11233,
    11238, 11207, 11208, 11220, 11232, 11204, 11218, 11219, 11230,
    11203, 11210, 11225, 11226, 11234, 11236, 11239, 11209, 11214,
    11228, 11223, 11224, 11229, 11235, 11206, 11221, 11237, 10031,
    10032, 10033, 10034, 10040, 10026, 10027, 10030, 10037, 10039,
    10029, 10035, 10023, 10024, 10025, 10021, 10028, 10044, 10128,
    10001, 10011, 10018, 10019, 10020, 10036, 10010, 10016, 10017,
    10022, 10012, 10013, 10014, 10002, 10003, 10009, 10004, 10005,
    10006, 10007, 10038, 10280, 11101, 11102, 11103, 11104, 11105,
    11106, 11368, 11369, 11370, 11372, 11373, 11377, 11378, 11354,
    11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363,
    11364, 11374, 11375, 11379, 11385, 11365, 11366, 11367, 11414,
    11415, 11416, 11417, 11418, 11419, 11420, 11421, 11412, 11423,
    11432, 11433, 11434, 11435, 11436, 11004, 11005, 11411, 11413,
    11422, 11426, 11427, 11428, 11429, 11691, 11692, 11693, 11694,
    11695, 11697, 10302, 10303, 10310, 10301, 10304, 10305, 10314,
    10306, 10307, 10308, 10309, 10312
}

crash_df['zip_code_numeric'] = pd.to_numeric(crash_df['zip_code'],
                                             errors='coerce')

invalid_zip_codes = crash_df[crash_df['zip_code_numeric'].isna() |
  ~crash_df['zip_code_numeric'].isin(valid_zips_codes)]['zip_code']

invalid_zip_freq = invalid_zip_codes.value_counts(dropna=False)
invalid_zip_freq.columns = ['zip_code', 'frequency']

invalid_zip_freq
```

```{python}
crash_df['zip_code'] = crash_df['zip_code'].replace(['NaN', '10065', '11249',
                                                     '10112', '11040'],
                                                     pd.NA)

crash_df['zip_code'].value_counts()
```

## Part g.

Are there missing in zip_code and borough? Do they always co-occur?

```{python}
missing_zip = crash_df['zip_code'].isnull().sum()
missing_borough = crash_df['borough'].isnull().sum()

missing_both = crash_df[['zip_code', 'borough']].isnull().all(axis=1).sum()

(missing_zip, missing_borough, missing_both)
```

The missing values in zip and borough do always co-occur.
The difference of 13 is from the invalid zip codes that were converted to NA.

## Part h.

Are there cases where zip_code and borough are missing but the geo codes are
not missing? If so, fill in zip_code and borough using the geo codes.

## Part i.

Is it redundant to keep both location and the longitude/latitude at
the NYC Open Data server?

Yes, it is redundant to keep both. We do not need two different
ways of storing the location data. Since it is easier to access the data
when logitude and latitude are in separate columns, we can get rid of the
location column which is containing both values in the same column.

```{python}
crash_df = crash_df.drop(columns = ['location'])
```

## Part j.

Check the frequency of crash_time by hour. Is there a matter of bad luck
at exactly midnight? How would you interpret this?

```{python}
crash_df.head()

crash_df['stop_index'] = crash_df['crash_time'].str.find(':')
crash_df['crash_time_hour'] = crash_df.apply(
  lambda x: x['crash_time'][:x['stop_index']], axis = 1)

crash_df['crash_time_hour'].value_counts()
```

```{python}
crash_df['crash_time'].value_counts()
```

The midnight hour is responsible for the second most amount of crashes.
The second at exactly midnight also appears to be a hotspot for crashes.
I would expect crashes to be more evenly distributed across all of the
seconds in a day. An explanation for this could be that the crash time is
more likely to be reported at hourly intervals since some people may find
it more practical. Those involved in a motor vehicle collision can be
distracted with their environment, which could lead them to give a rough
estimate of when the crash occured.

## Part k.

Are the number of persons killed/injured the summation of the numbers of
pedestrians, cyclist, and motorists killed/injured? If so, is it redundant
to keep these two columns at the NYC Open Data server?

```{python}
crash_df['injured_sum'] = crash_df['number_of_pedestrians_injured'] + \
                          crash_df['number_of_cyclist_injured'] + \
                          crash_df['number_of_motorist_injured']

crash_df['injured_verify'] = crash_df['number_of_persons_injured'] == \
                             crash_df['injured_sum']

crash_df['injured_verify'].value_counts()
```

```{python}
crash_df['killed_sum'] = crash_df['number_of_pedestrians_killed'] + \
                          crash_df['number_of_cyclist_killed'] + \
                          crash_df['number_of_motorist_killed']

crash_df['killed_verify'] = crash_df['number_of_persons_killed'] == \
                             crash_df['killed_sum']

crash_df['killed_verify'].value_counts()
```

Yes, the number of persons injured/killed is the summation of the
pedestrians, cyclists, and motorists columns. It is redundant to keep
these summartion columns since they can be derived from other columns and
they take up space. However, the pedestrians, cyclists, and motorists columns
should not be removed since they give information about the individual crash
that we otherwise would not have.

```{python}
crash_df = crash_df.drop(columns = ['number_of_persons_injured',
                                    'number_of_persons_killed'])
```

## Part l.

Print the whole frequency table of contributing_factor_vehicle_1.
Convert lower cases to uppercases and check the frequencies again.

```{python}
cf1_freq = crash_df['contributing_factor_vehicle_1'].value_counts(dropna=False)
cf1_freq.columns = ['contributing_factor_vehicle_1', 'count']
cf1_freq
```

```{python}
crash_df['contributing_factor_vehicle_1'] = crash_df['contributing_factor_vehicle_1'].str.upper()

cf1_freq = crash_df['contributing_factor_vehicle_1'].value_counts(dropna=False)
cf1_freq.columns = ['contributing_factor_vehicle_1', 'count']
cf1_freq
```

## Part m.

Provided an opportunity to meet the data provider, what suggestions
would you make based on your data exploration experience?